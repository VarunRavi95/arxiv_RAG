{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('filtered_arxiv_papers_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['categories']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = np.load('embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (uncomment if needed)\n",
    "# !pip install transformers datasets faiss-cpu sentence-transformers torch tqdm pandas numpy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# ==============================\n",
    "# Configuration Variables\n",
    "# ==============================\n",
    "\n",
    "# Path to the filtered arXiv metadata JSON file\n",
    "INPUT_JSON = 'arxiv-metadata-oai-snapshot.json'  # Replace with your actual file path\n",
    "\n",
    "# Path to save the FAISS index\n",
    "FAISS_INDEX_PATH = 'faiss_index.index'\n",
    "\n",
    "# Path to save the metadata mapping\n",
    "METADATA_MAPPING_PATH = 'metadata_mapping.json'\n",
    "\n",
    "# Number of top similar documents to retrieve\n",
    "TOP_K = 5  # You can adjust this number as needed\n",
    "\n",
    "# Directory to store logs\n",
    "LOG_FILE = 'rag_system_macos.log'\n",
    "\n",
    "# Path to embeddings file\n",
    "EMBEDDINGS_PATH = 'embeddings.npy'\n",
    "\n",
    "# Embedding model name\n",
    "EMBEDDING_MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "# Generation model name (using an instruction-tuned model)\n",
    "GENERATION_MODEL_NAME = 'google/flan-t5-small'  # Use 'google/flan-t5-small' if resource constraints\n",
    "\n",
    "# Maximum input length for the generation model\n",
    "MAX_INPUT_LENGTH = 512\n",
    "\n",
    "# Reserve tokens for the answer\n",
    "RESERVED_TOKENS = 100  # Adjust based on expected answer length\n",
    "\n",
    "# ==============================\n",
    "# Setup Logging\n",
    "# ==============================\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=LOG_FILE,\n",
    "    filemode='a',\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# Function Definitions\n",
    "# ==============================\n",
    "\n",
    "def load_metadata(json_path, categories):\n",
    "    \"\"\"\n",
    "    Loads and filters the arXiv metadata from a JSON file based on specified categories.\n",
    "    \n",
    "    Parameters:\n",
    "        json_path (str): Path to the arXiv JSON dataset.\n",
    "        categories (list): List of categories to filter by.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing filtered metadata.\n",
    "    \"\"\"\n",
    "    papers = []\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            for line in tqdm(f, desc=\"Loading and Filtering Papers\"):\n",
    "                try:\n",
    "                    paper = json.loads(line)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue  # Skip malformed lines\n",
    "                \n",
    "                paper_categories = paper.get('categories', '').split()\n",
    "                if not set(paper_categories).intersection(set(categories)):\n",
    "                    continue  # Skip papers not in the desired categories\n",
    "                \n",
    "                arxiv_id = paper.get('id', '')\n",
    "                title = paper.get('title', '').replace('\\n', ' ').strip()\n",
    "                authors = paper.get('authors', '').strip()  # Corrected authors field\n",
    "                abstract = paper.get('abstract', '').replace('\\n', ' ').strip()\n",
    "                published_date = paper.get('published', '')\n",
    "                categories_str = ', '.join(paper_categories)\n",
    "                arxiv_url = paper.get('link', '')\n",
    "                journal_ref = paper.get('journal-ref', '')\n",
    "                comment = paper.get('comment', '')\n",
    "                doi = paper.get('doi', '')\n",
    "                \n",
    "                papers.append({\n",
    "                    'arxiv_id': arxiv_id,\n",
    "                    'title': title,\n",
    "                    'authors': authors,\n",
    "                    'abstract': abstract,\n",
    "                    'published_date': published_date,\n",
    "                    'categories': categories_str,\n",
    "                    'arxiv_url': arxiv_url,\n",
    "                    'journal_ref': journal_ref,\n",
    "                    'comment': comment,\n",
    "                    'doi': doi\n",
    "                })\n",
    "        df = pd.DataFrame(papers)\n",
    "        print(f\"Loaded and filtered metadata for {len(df)} papers.\")\n",
    "        logging.info(f\"Loaded and filtered metadata for {len(df)} papers.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading metadata: {e}\")\n",
    "        print(f\"Error loading metadata: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def preprocess_text(row):\n",
    "    \"\"\"\n",
    "    Preprocesses the text by combining relevant fields.\n",
    "    \n",
    "    Parameters:\n",
    "        row (pd.Series): A row from the dataframe.\n",
    "    \n",
    "    Returns:\n",
    "        str: Combined text.\n",
    "    \"\"\"\n",
    "    title = row['title'] if pd.notna(row['title']) else ''\n",
    "    abstract = row['abstract'] if pd.notna(row['abstract']) else ''\n",
    "    categories = row['categories'] if pd.notna(row['categories']) else ''\n",
    "    return f\"{title}:{abstract}\"\n",
    "\n",
    "def generate_embeddings(texts, model_name='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "    \"\"\"\n",
    "    Generates embeddings for a list of texts using SentenceTransformer.\n",
    "    \n",
    "    Parameters:\n",
    "        texts (list of str): List of text strings.\n",
    "        model_name (str): SentenceTransformer model name.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Array of embeddings.\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n",
    "    return embeddings\n",
    "\n",
    "def build_faiss_index(embeddings, index_path):\n",
    "    \"\"\"\n",
    "    Builds and saves a FAISS index from embeddings.\n",
    "    \n",
    "    Parameters:\n",
    "        embeddings (np.ndarray): Array of embeddings.\n",
    "        index_path (str): Path to save the FAISS index.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Normalize embeddings\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        \n",
    "        # Initialize FAISS index\n",
    "        index = faiss.IndexFlatIP(embeddings.shape[1])  # Inner Product for cosine similarity\n",
    "        index.add(embeddings)\n",
    "        faiss.write_index(index, index_path)\n",
    "        print(f\"FAISS index built and saved to {index_path}\")\n",
    "        logging.info(f\"FAISS index built and saved to {index_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error building FAISS index: {e}\")\n",
    "        print(f\"Error building FAISS index: {e}\")\n",
    "\n",
    "def save_metadata_mapping(df, mapping_path):\n",
    "    \"\"\"\n",
    "    Saves the metadata mapping to a JSON file.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing the metadata.\n",
    "        mapping_path (str): Path to save the mapping.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mapping = df.to_dict(orient='records')\n",
    "        with open(mapping_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(mapping, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Metadata mapping saved to {mapping_path}\")\n",
    "        logging.info(f\"Metadata mapping saved to {mapping_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving metadata mapping: {e}\")\n",
    "        print(f\"Error saving metadata mapping: {e}\")\n",
    "\n",
    "def load_faiss_index(index_path):\n",
    "    \"\"\"\n",
    "    Loads a FAISS index from a file.\n",
    "    \n",
    "    Parameters:\n",
    "        index_path (str): Path to the FAISS index file.\n",
    "    \n",
    "    Returns:\n",
    "        faiss.Index: Loaded FAISS index.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        index = faiss.read_index(index_path)\n",
    "        print(f\"FAISS index loaded from {index_path}\")\n",
    "        logging.info(f\"FAISS index loaded from {index_path}\")\n",
    "        return index\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading FAISS index: {e}\")\n",
    "        print(f\"Error loading FAISS index: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_metadata_mapping(mapping_path):\n",
    "    \"\"\"\n",
    "    Loads metadata mapping from a JSON file.\n",
    "    \n",
    "    Parameters:\n",
    "        mapping_path (str): Path to the metadata mapping JSON file.\n",
    "    \n",
    "    Returns:\n",
    "        list of dict: Metadata mapping.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(mapping_path, 'r', encoding='utf-8') as f:\n",
    "            metadata_mapping = json.load(f)\n",
    "        print(f\"Metadata mapping loaded from {mapping_path}\")\n",
    "        logging.info(f\"Metadata mapping loaded from {mapping_path}\")\n",
    "        return metadata_mapping\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading metadata mapping: {e}\")\n",
    "        print(f\"Error loading metadata mapping: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_generation_model(model_name='google/flan-t5-base'):\n",
    "    \"\"\"\n",
    "    Loads the generation model and tokenizer.\n",
    "    \n",
    "    Parameters:\n",
    "        model_name (str): Hugging Face model name.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (tokenizer, model, device)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        \n",
    "        # Move model to GPU (MPS on macOS if available)\n",
    "        if torch.backends.mps.is_available():\n",
    "            device = torch.device(\"mps\")\n",
    "            model.to(device)\n",
    "            print(\"Generation model loaded on macOS GPU (MPS).\")\n",
    "        elif torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "            model.to(device)\n",
    "            print(\"Generation model loaded on CUDA GPU.\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            model.to(device)\n",
    "            print(\"Generation model loaded on CPU.\")\n",
    "        \n",
    "        logging.info(f\"Generation model '{model_name}' loaded successfully on {device}.\")\n",
    "        return tokenizer, model, device\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading generation model: {e}\")\n",
    "        print(f\"Error loading generation model: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def query_faiss(index, query_embedding, top_k=5):\n",
    "    \"\"\"\n",
    "    Queries the FAISS index to retrieve top_k similar embeddings.\n",
    "    \n",
    "    Parameters:\n",
    "        index (faiss.Index): FAISS index.\n",
    "        query_embedding (np.ndarray): Embedding of the query.\n",
    "        top_k (int): Number of top results to retrieve.\n",
    "    \n",
    "    Returns:\n",
    "        list of int: Indices of the top_k similar embeddings.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Normalize the query embedding\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        \n",
    "        # Perform the search\n",
    "        distances, indices = index.search(query_embedding, top_k)\n",
    "        return indices[0]\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error querying FAISS index: {e}\")\n",
    "        print(f\"Error querying FAISS index: {e}\")\n",
    "        return []\n",
    "\n",
    "def generate_answer(tokenizer, model, device, prompt):\n",
    "    \"\"\"\n",
    "    Generates an answer based on the prompt using the generation model.\n",
    "    \n",
    "    Parameters:\n",
    "        tokenizer: Tokenizer of the generation model.\n",
    "        model: Generation model.\n",
    "        device: Device where the model is loaded.\n",
    "        prompt (str): The input prompt.\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated answer.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        inputs = tokenizer(prompt, return_tensors='pt', max_length=MAX_INPUT_LENGTH, truncation=True).to(device)\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_length=MAX_INPUT_LENGTH,\n",
    "            num_beams=5,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error generating answer: {e}\")\n",
    "        print(f\"Error generating answer: {e}\")\n",
    "        return \"I'm sorry, I couldn't generate an answer for your query.\"\n",
    "\n",
    "# ==============================\n",
    "# Main Execution\n",
    "# ==============================\n",
    "\n",
    "def main():\n",
    "    # Step 1: Load and filter the metadata\n",
    "    categories = ['cs.AI', 'cs.LG', 'cs.CV', 'cs.CL', 'stat.ML']\n",
    "    df = load_metadata(INPUT_JSON, categories)\n",
    "    if df.empty:\n",
    "        print(\"No data to process. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Preprocess text for embeddings\n",
    "    print(\"Preprocessing text for embeddings...\")\n",
    "    df['combined_text'] = df.apply(preprocess_text, axis=1)\n",
    "    combined_texts = df['combined_text'].tolist()\n",
    "    \n",
    "    # Step 3: Generate embeddings\n",
    "    print(\"Generating embeddings...\")\n",
    "    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "    # Check if embeddings file exists\n",
    "    if Path(EMBEDDINGS_PATH).exists():\n",
    "        print(f\"Embeddings file '{EMBEDDINGS_PATH}' found. Loading embeddings...\")\n",
    "        embeddings = np.load(EMBEDDINGS_PATH)\n",
    "    else:\n",
    "        embeddings = generate_embeddings(combined_texts, model_name=EMBEDDING_MODEL_NAME)\n",
    "        np.save(EMBEDDINGS_PATH, embeddings)\n",
    "        print(f\"Embeddings saved to {EMBEDDINGS_PATH}\")\n",
    "    \n",
    "    # Step 4: Build and save FAISS index\n",
    "    print(\"Building FAISS index...\")\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    index = faiss.IndexFlatIP(embeddings.shape[1])  # Inner Product for cosine similarity\n",
    "    index.add(embeddings)\n",
    "    faiss.write_index(index, FAISS_INDEX_PATH)\n",
    "    print(f\"FAISS index built and saved to {FAISS_INDEX_PATH}\")\n",
    "    logging.info(f\"FAISS index built and saved to {FAISS_INDEX_PATH}\")\n",
    "    \n",
    "    # Step 5: Save metadata mapping\n",
    "    print(\"Saving metadata mapping...\")\n",
    "    mapping = df.to_dict(orient='records')\n",
    "    with open(METADATA_MAPPING_PATH, 'w', encoding='utf-8') as f:\n",
    "        json.dump(mapping, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Metadata mapping saved to {METADATA_MAPPING_PATH}\")\n",
    "    logging.info(f\"Metadata mapping saved to {METADATA_MAPPING_PATH}\")\n",
    "    \n",
    "    # Step 6: Load FAISS index and metadata mapping\n",
    "    index = load_faiss_index(FAISS_INDEX_PATH)\n",
    "    if index is None:\n",
    "        print(\"Failed to load FAISS index. Exiting.\")\n",
    "        return\n",
    "    metadata_mapping = load_metadata_mapping(METADATA_MAPPING_PATH)\n",
    "    if not metadata_mapping:\n",
    "        print(\"Failed to load metadata mapping. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Step 7: Load generation model\n",
    "    print(\"Loading generation model...\")\n",
    "    tokenizer, model, device = load_generation_model(model_name=GENERATION_MODEL_NAME)\n",
    "    if tokenizer is None or model is None:\n",
    "        print(\"Failed to load generation model. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Step 8: Interactive Querying\n",
    "    print(\"\\nRAG System is ready. You can now ask questions related to AI papers.\")\n",
    "    print(\"Type 'exit' to quit.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_query = input(\"Your Question: \")\n",
    "        if user_query.lower() in ['exit', 'quit']:\n",
    "            print(\"Exiting RAG System. Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Step 8a: Generate embedding for the query\n",
    "        query_embedding = embedding_model.encode([user_query], convert_to_numpy=True).astype('float32')\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        \n",
    "        # Step 8b: Retrieve top_k similar papers\n",
    "        top_indices = query_faiss(index, query_embedding, top_k=TOP_K)\n",
    "        retrieved_papers = [metadata_mapping[idx] for idx in top_indices]\n",
    "        \n",
    "        # Step 8c: Prepare context for generation\n",
    "        # Limit context length to fit within model's maximum input length\n",
    "        max_context_length = MAX_INPUT_LENGTH - RESERVED_TOKENS\n",
    "        context_texts = []\n",
    "        total_tokens = 0\n",
    "        \n",
    "        for paper in retrieved_papers:\n",
    "            text = f\"{paper['title']}: {paper['abstract']}\"\n",
    "            encoded_text = tokenizer.encode(text, truncation=True, max_length=max_context_length, add_special_tokens=False)\n",
    "            text_length = len(encoded_text)\n",
    "            # if total_tokens + text_length > max_context_length:\n",
    "                # break\n",
    "            context_texts.append(text)\n",
    "            total_tokens += text_length\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_texts)\n",
    "        \n",
    "        # Optional: Print the context for debugging\n",
    "        # print(f'Context:\\n{context}')\n",
    "        \n",
    "        # Step 8d: Create prompt for generation\n",
    "        prompt = (\n",
    "            f\"Please provide a concise and informative answer to the question based on the context below.\\n\\n\"\n",
    "            f\"Context:\\n{context}\\n\\n\"\n",
    "            f\"Question: {user_query}\\n\"\n",
    "            f\"Answer:\"\n",
    "        )\n",
    "        \n",
    "        # Optional: Print the prompt for debugging\n",
    "        print(f'Prompt:\\n{prompt}')\n",
    "        \n",
    "        # Step 8e: Generate answer\n",
    "        answer = generate_answer(tokenizer, model, device, prompt)\n",
    "        print(f\"\\nAnswer:\\n{answer}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (uncomment if needed)\n",
    "# !pip install transformers datasets faiss-cpu sentence-transformers torch tqdm pandas numpy\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import logging\n",
    "from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "import torch\n",
    "from pathlib import Path  # Added import for Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Configuration Variables\n",
    "# ==============================\n",
    "\n",
    "INPUT_JSON = 'arxiv-metadata-oai-snapshot.json'  # Path to your arXiv metadata JSON\n",
    "CATEGORIES = ['cs.AI', 'cs.LG', 'cs.CV', 'cs.CL', 'stat.ML']  # Categories of interest\n",
    "EMBEDDING_MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'  # Embedding model\n",
    "FAISS_INDEX_PATH = 'faiss_index.index'\n",
    "METADATA_MAPPING_PATH = 'metadata_mapping.json'\n",
    "EMBEDDINGS_PATH = 'embeddings.npy'\n",
    "LOG_FILE = 'knowledge_base_preparation.log'\n",
    "QA_DATASET_NAME = 'qasper'  # Hugging Face dataset name\n",
    "\n",
    "FINE_TUNED_MODEL_PATH = './rag-finetuned-qasper'\n",
    "\n",
    "# ==============================\n",
    "# Setup Logging\n",
    "# ==============================\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=LOG_FILE,\n",
    "    filemode='a',\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# Function Definitions\n",
    "# ==============================\n",
    "\n",
    "def load_and_filter_arxiv(json_path, categories):\n",
    "    papers = []\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            for line in tqdm(f, desc=\"Loading and Filtering Papers\"):\n",
    "                try:\n",
    "                    paper = json.loads(line)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "                paper_categories = paper.get('categories', '').split()\n",
    "                if not set(paper_categories).intersection(set(categories)):\n",
    "                    continue\n",
    "                arxiv_id = paper.get('id', '')\n",
    "                title = paper.get('title', '').replace('\\n', ' ').strip()\n",
    "                authors = ', '.join(paper.get('authors', []))\n",
    "                abstract = paper.get('abstract', '').replace('\\n', ' ').strip()\n",
    "                published_date = paper.get('published', '')\n",
    "                categories_str = ', '.join(paper_categories)\n",
    "                arxiv_url = paper.get('link', '')\n",
    "                journal_ref = paper.get('journal-ref', '')\n",
    "                comment = paper.get('comment', '')\n",
    "                doi = paper.get('doi', '')\n",
    "                papers.append({\n",
    "                    'arxiv_id': arxiv_id,\n",
    "                    'title': title,\n",
    "                    'authors': authors,\n",
    "                    'abstract': abstract,\n",
    "                    'published_date': published_date,\n",
    "                    'categories': categories_str,\n",
    "                    'arxiv_url': arxiv_url,\n",
    "                    'journal_ref': journal_ref,\n",
    "                    'comment': comment,\n",
    "                    'doi': doi\n",
    "                })\n",
    "        df = pd.DataFrame(papers)\n",
    "        logging.info(f\"Loaded and filtered metadata for {len(df)} papers.\")\n",
    "        print(f\"Loaded and filtered metadata for {len(df)} papers.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading metadata: {e}\")\n",
    "        print(f\"Error loading metadata: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def preprocess_documents(df):\n",
    "    combined_texts = []\n",
    "    for _, row in df.iterrows():\n",
    "        title = row['title'] if pd.notna(row['title']) else ''\n",
    "        abstract = row['abstract'] if pd.notna(row['abstract']) else ''\n",
    "        categories = row['categories'] if pd.notna(row['categories']) else ''\n",
    "        combined = f\"Title: {title}\\nAbstract: {abstract}\\nCategories: {categories}\"\n",
    "        combined_texts.append(combined)\n",
    "    return combined_texts\n",
    "\n",
    "def generate_embeddings(texts, model_name):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n",
    "    return embeddings\n",
    "\n",
    "def build_faiss_index(embeddings, index_path):\n",
    "    try:\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "        index.add(embeddings)\n",
    "        faiss.write_index(index, index_path)\n",
    "        logging.info(f\"FAISS index built and saved to {index_path}\")\n",
    "        print(f\"FAISS index built and saved to {index_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error building FAISS index: {e}\")\n",
    "        print(f\"Error building FAISS index: {e}\")\n",
    "\n",
    "def save_metadata_mapping(df, mapping_path):\n",
    "    try:\n",
    "        mapping = df.to_dict(orient='records')\n",
    "        with open(mapping_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(mapping, f, ensure_ascii=False, indent=4)\n",
    "        logging.info(f\"Metadata mapping saved to {mapping_path}\")\n",
    "        print(f\"Metadata mapping saved to {mapping_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving metadata mapping: {e}\")\n",
    "        print(f\"Error saving metadata mapping: {e}\")\n",
    "\n",
    "def load_faiss_index(index_path):\n",
    "    try:\n",
    "        index = faiss.read_index(index_path)\n",
    "        print(f\"FAISS index loaded from {index_path}\")\n",
    "        logging.info(f\"FAISS index loaded from {index_path}\")\n",
    "        return index\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading FAISS index: {e}\")\n",
    "        print(f\"Error loading FAISS index: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_metadata_mapping(mapping_path):\n",
    "    try:\n",
    "        with open(mapping_path, 'r', encoding='utf-8') as f:\n",
    "            metadata_mapping = json.load(f)\n",
    "        print(f\"Metadata mapping loaded from {mapping_path}\")\n",
    "        logging.info(f\"Metadata mapping loaded from {mapping_path}\")\n",
    "        return metadata_mapping\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading metadata mapping: {e}\")\n",
    "        print(f\"Error loading metadata mapping: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_qa_dataset(dataset_name):\n",
    "    try:\n",
    "        dataset = load_dataset(dataset_name)\n",
    "        print(f\"Loaded QA dataset '{dataset_name}' with {len(dataset['train'])} samples.\")\n",
    "        logging.info(f\"Loaded QA dataset '{dataset_name}' with {len(dataset['train'])} samples.\")\n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading QA dataset '{dataset_name}': {e}\")\n",
    "        print(f\"Error loading QA dataset '{dataset_name}': {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_rag(qa_dataset, faiss_index_path, metadata_mapping_path):\n",
    "    try:\n",
    "        tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-sequence-nq\")\n",
    "        retriever = RagRetriever.from_pretrained(\n",
    "            \"facebook/rag-sequence-nq\",\n",
    "            index_name=\"custom\",\n",
    "            passages_path=\"passages_dataset\",  # Updated to point to the passages dataset directory\n",
    "            use_dummy_dataset=False\n",
    "        )\n",
    "        model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-sequence-nq\")\n",
    "        \n",
    "        # Load FAISS index and set passages\n",
    "        retriever.index = faiss.read_index(faiss_index_path)\n",
    "        with open(metadata_mapping_path, 'r', encoding='utf-8') as f:\n",
    "            metadata = json.load(f)\n",
    "        retriever.set_passages(metadata)\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=FINE_TUNED_MODEL_PATH,\n",
    "            per_device_train_batch_size=2,\n",
    "            per_device_eval_batch_size=2,\n",
    "            num_train_epochs=3,\n",
    "            learning_rate=5e-5,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            save_steps=500,\n",
    "            save_total_limit=2,\n",
    "            logging_steps=100,\n",
    "            fp16=True,\n",
    "            dataloader_num_workers=4,\n",
    "        )\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=qa_dataset['train'],\n",
    "            eval_dataset=qa_dataset['validation'] if 'validation' in qa_dataset else qa_dataset['train'],\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        trainer.save_model(FINE_TUNED_MODEL_PATH)\n",
    "        tokenizer.save_pretrained(FINE_TUNED_MODEL_PATH)\n",
    "        retriever.save_pretrained(FINE_TUNED_MODEL_PATH)\n",
    "        logging.info(f\"Fine-tuned RAG model saved to {FINE_TUNED_MODEL_PATH}\")\n",
    "        print(f\"Fine-tuned RAG model saved to {FINE_TUNED_MODEL_PATH}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during fine-tuning: {e}\")\n",
    "        print(f\"Error during fine-tuning: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_finetuned_rag(model_path):\n",
    "    try:\n",
    "        tokenizer = RagTokenizer.from_pretrained(model_path)\n",
    "        retriever = RagRetriever.from_pretrained(\n",
    "            model_path,\n",
    "            index_name=\"custom\",\n",
    "            passages_path=\"passages_dataset\",  # Updated to point to the passages dataset directory\n",
    "            use_dummy_dataset=False\n",
    "        )\n",
    "        model = RagSequenceForGeneration.from_pretrained(model_path)\n",
    "        \n",
    "        # Load FAISS index and set passages\n",
    "        retriever.index = faiss.read_index(FAISS_INDEX_PATH)\n",
    "        with open(METADATA_MAPPING_PATH, 'r', encoding='utf-8') as f:\n",
    "            metadata = json.load(f)\n",
    "        retriever.set_passages(metadata)\n",
    "        \n",
    "        return tokenizer, retriever, model\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading fine-tuned RAG model: {e}\")\n",
    "        print(f\"Error loading fine-tuned RAG model: {e}\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(tokenizer, retriever, model, query, device, max_new_tokens=200):\n",
    "    try:\n",
    "        inputs = tokenizer(query, return_tensors=\"pt\").to(device)\n",
    "        generated = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        answer = tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error generating answer: {e}\")\n",
    "        print(f\"Error generating answer: {e}\")\n",
    "        return \"I'm sorry, I couldn't generate an answer for your query.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_querying(tokenizer, retriever, model, device):\n",
    "    print(\"\\nRAG System is ready. You can now ask questions related to AI papers.\")\n",
    "    print(\"Type 'exit' to quit.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_query = input(\"Your Question: \")\n",
    "        if user_query.lower() in ['exit', 'quit']:\n",
    "            print(\"Exiting RAG System. Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        answer = answer_query(tokenizer, retriever, model, user_query, device)\n",
    "        print(f\"\\nAnswer:\\n{answer}\\n\")\n",
    "\n",
    "def main():\n",
    "    # Step 1: Prepare Knowledge Base\n",
    "    df = load_and_filter_arxiv(INPUT_JSON, CATEGORIES)\n",
    "    if df.empty:\n",
    "        print(\"No data loaded. Exiting.\")\n",
    "        return\n",
    "    combined_texts = preprocess_documents(df)\n",
    "    \n",
    "    # Check if embeddings file exists\n",
    "    if Path(EMBEDDINGS_PATH).exists():\n",
    "        print(f\"Embeddings file '{EMBEDDINGS_PATH}' found. Loading embeddings...\")\n",
    "        embeddings = np.load(EMBEDDINGS_PATH)\n",
    "    else:\n",
    "        embeddings = generate_embeddings(combined_texts, EMBEDDING_MODEL_NAME)\n",
    "        np.save(EMBEDDINGS_PATH, embeddings)\n",
    "        print(f\"Embeddings saved to {EMBEDDINGS_PATH}\")\n",
    "    \n",
    "    build_faiss_index(embeddings, FAISS_INDEX_PATH)\n",
    "    save_metadata_mapping(df, METADATA_MAPPING_PATH)\n",
    "    \n",
    "    # Create and save passages dataset\n",
    "    print(\"Creating passages dataset...\")\n",
    "    passages_dataset = Dataset.from_dict({\n",
    "        'title': df['title'].tolist(),\n",
    "        'abstract': df['abstract'].tolist(),\n",
    "        'categories': df['categories'].tolist(),\n",
    "        'text': combined_texts\n",
    "    })\n",
    "    passages_dataset.save_to_disk('passages_dataset')\n",
    "    print(\"Passages dataset saved to 'passages_dataset' directory.\")\n",
    "    \n",
    "    # Step 2: Load QA Dataset\n",
    "    qa_dataset = load_qa_dataset(QA_DATASET_NAME)\n",
    "    if qa_dataset is None:\n",
    "        print(\"Failed to load QA dataset. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Step 3: Fine-Tune RAG Model\n",
    "    fine_tune_rag(qa_dataset, FAISS_INDEX_PATH, METADATA_MAPPING_PATH)\n",
    "    \n",
    "    # Step 4: Inference\n",
    "    tokenizer, retriever, model = load_finetuned_rag(FINE_TUNED_MODEL_PATH)\n",
    "    if tokenizer is None or retriever is None or model is None:\n",
    "        print(\"Failed to load the fine-tuned RAG model. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        model.to(device)\n",
    "        print(\"Using macOS GPU (MPS) for inference.\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        model.to(device)\n",
    "        print(\"Using CUDA GPU for inference.\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        model.to(device)\n",
    "        print(\"Using CPU for inference.\")\n",
    "    \n",
    "    interactive_querying(tokenizer, retriever, model, device)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (uncomment if needed)\n",
    "# !pip install transformers datasets peft accelerate torch tqdm pandas numpy\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# ==============================\n",
    "# Configuration Variables\n",
    "# ==============================\n",
    "\n",
    "# Path to the filtered arXiv metadata JSON file\n",
    "INPUT_JSON = 'arxiv-metadata-oai-snapshot.json'  # Replace with your actual file path\n",
    "\n",
    "# Path to save the fine-tuned model\n",
    "OUTPUT_DIR = 'fine_tuned_gpt_neo'\n",
    "\n",
    "# Number of training epochs\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "# Learning rate\n",
    "LEARNING_RATE = 5e-5\n",
    "\n",
    "# Number of top similar documents to retrieve initially\n",
    "INITIAL_TOP_K = 50\n",
    "\n",
    "# Number of documents after re-ranking to include in context\n",
    "TOP_N = 5\n",
    "\n",
    "# Batch size per device during training\n",
    "BATCH_SIZE = 1  # Adjust based on memory availability\n",
    "\n",
    "# Embedding model name\n",
    "EMBEDDING_MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "# Generation model name (using an instruction-tuned model)\n",
    "GENERATION_MODEL_NAME = 'google/flan-t5-small'  # Change to 'google/flan-t5-base' if feasible\n",
    "\n",
    "# Maximum input length for the generation model\n",
    "MAX_INPUT_LENGTH = 512\n",
    "\n",
    "# Reserve tokens for the answer and prompt\n",
    "RESERVED_TOKENS = 100\n",
    "\n",
    "# Categories to filter arXiv papers\n",
    "CATEGORIES = ['cs.AI', 'cs.LG', 'cs.CV', 'cs.CL', 'stat.ML']\n",
    "\n",
    "# ==============================\n",
    "# Function Definitions\n",
    "# ==============================\n",
    "\n",
    "def load_metadata(json_path, categories):\n",
    "    \"\"\"\n",
    "    Loads and filters the arXiv metadata from a JSON file based on specified categories.\n",
    "\n",
    "    Parameters:\n",
    "        json_path (str): Path to the arXiv JSON dataset.\n",
    "        categories (list): List of categories to filter by.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing filtered metadata.\n",
    "    \"\"\"\n",
    "    papers = []\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            for line in tqdm(f, desc=\"Loading and Filtering Papers\"):\n",
    "                try:\n",
    "                    paper = json.loads(line)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue  # Skip malformed lines\n",
    "\n",
    "                paper_categories = paper.get('categories', '').split()\n",
    "                if not set(paper_categories).intersection(set(categories)):\n",
    "                    continue  # Skip papers not in the desired categories\n",
    "\n",
    "                arxiv_id = paper.get('id', '')\n",
    "                title = paper.get('title', '').replace('\\n', ' ').strip()\n",
    "                authors = ', '.join(paper.get('authors', []))\n",
    "                abstract = paper.get('abstract', '').replace('\\n', ' ').strip()\n",
    "\n",
    "                papers.append({\n",
    "                    'arxiv_id': arxiv_id,\n",
    "                    'title': title,\n",
    "                    'authors': authors,\n",
    "                    'abstract': abstract\n",
    "                })\n",
    "        df = pd.DataFrame(papers)\n",
    "        print(f\"Loaded and filtered metadata for {len(df)} papers.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading metadata: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def preprocess_text(row):\n",
    "    \"\"\"\n",
    "    Preprocesses the text by combining relevant fields.\n",
    "\n",
    "    Parameters:\n",
    "        row (pd.Series): A row from the dataframe.\n",
    "\n",
    "    Returns:\n",
    "        str: Combined text.\n",
    "    \"\"\"\n",
    "    title = row['title'] if pd.notna(row['title']) else ''\n",
    "    abstract = row['abstract'] if pd.notna(row['abstract']) else ''\n",
    "    return f\"Title: {title}\\nAbstract: {abstract}\"\n",
    "\n",
    "def generate_embeddings(texts, model_name='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "    \"\"\"\n",
    "    Generates embeddings for a list of texts using SentenceTransformer.\n",
    "\n",
    "    Parameters:\n",
    "        texts (list of str): List of text strings.\n",
    "        model_name (str): SentenceTransformer model name.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of embeddings.\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n",
    "    return embeddings\n",
    "\n",
    "def load_model_with_lora(model_name, device):\n",
    "    \"\"\"\n",
    "    Loads the pretrained model with LoRA configuration for fine-tuning.\n",
    "\n",
    "    Parameters:\n",
    "        model_name (str): Hugging Face model name.\n",
    "        device (torch.device): Device to load the model onto.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (tokenizer, model)\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16 if device.type == 'mps' else torch.float32\n",
    "    )\n",
    "\n",
    "    # Define LoRA configuration\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "    )\n",
    "\n",
    "    # Apply LoRA to the model\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "def get_training_arguments(output_dir, per_device_train_batch_size=1, num_train_epochs=3):\n",
    "    \"\"\"\n",
    "    Defines the training arguments for the Trainer.\n",
    "\n",
    "    Parameters:\n",
    "        output_dir (str): Directory to save the fine-tuned model.\n",
    "        per_device_train_batch_size (int): Batch size per device.\n",
    "        num_train_epochs (int): Number of training epochs.\n",
    "\n",
    "    Returns:\n",
    "        TrainingArguments: Hugging Face TrainingArguments instance.\n",
    "    \"\"\"\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=8,  # Adjust based on GPU memory\n",
    "        evaluation_strategy=\"no\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=100,\n",
    "        fp16=True if torch.cuda.is_available() or torch.backends.mps.is_available() else False,\n",
    "        save_total_limit=2,\n",
    "        dataloader_num_workers=4,\n",
    "        optim=\"adamw_torch\",\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=0.01,\n",
    "        report_to=\"none\",  # Disable logging to external systems\n",
    "    )\n",
    "    return training_args\n",
    "\n",
    "def prepare_dataset(df, tokenizer, max_length=512):\n",
    "    \"\"\"\n",
    "    Prepares the dataset for training by tokenizing the text.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing the dataset.\n",
    "        tokenizer: Tokenizer instance.\n",
    "        max_length (int): Maximum sequence length.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: Hugging Face Dataset object.\n",
    "    \"\"\"\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples['text'],\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "    # Combine title and abstract\n",
    "    df['text'] = df.apply(preprocess_text, axis=1)\n",
    "    dataset = Dataset.from_pandas(df[['text']])\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "    return tokenized_dataset\n",
    "\n",
    "def get_trainer(model, tokenizer, training_args, train_dataset):\n",
    "    \"\"\"\n",
    "    Initializes the Hugging Face Trainer.\n",
    "\n",
    "    Parameters:\n",
    "        model: The model to train.\n",
    "        tokenizer: The tokenizer.\n",
    "        training_args: TrainingArguments instance.\n",
    "        train_dataset: The training dataset.\n",
    "\n",
    "    Returns:\n",
    "        Trainer: Hugging Face Trainer instance.\n",
    "    \"\"\"\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "def save_fine_tuned_model(trainer, output_dir):\n",
    "    \"\"\"\n",
    "    Saves the fine-tuned model and tokenizer.\n",
    "\n",
    "    Parameters:\n",
    "        trainer: Hugging Face Trainer instance.\n",
    "        output_dir (str): Directory to save the model.\n",
    "    \"\"\"\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer = trainer.tokenizer\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Fine-tuned model saved to {output_dir}\")\n",
    "\n",
    "# ==============================\n",
    "# Main Execution\n",
    "# ==============================\n",
    "\n",
    "def main():\n",
    "    # Detect device\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using MacOS GPU (MPS).\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"Using CUDA GPU.\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU.\")\n",
    "\n",
    "    # Step 1: Load and filter the metadata\n",
    "    df = load_metadata(INPUT_JSON, CATEGORIES)\n",
    "    if df.empty:\n",
    "        print(\"No data to process. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Step 2: Preprocess text for embeddings\n",
    "    print(\"Preprocessing text for embeddings...\")\n",
    "    df['combined_text'] = df.apply(preprocess_text, axis=1)\n",
    "    combined_texts = df['combined_text'].tolist()\n",
    "\n",
    "    # Step 3: Generate embeddings\n",
    "    print(\"Generating embeddings...\")\n",
    "    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "    # Check if embeddings file exists\n",
    "    if Path('embeddings.npy').exists():\n",
    "        print(f\"Embeddings file 'embeddings.npy' found. Loading embeddings...\")\n",
    "        embeddings = np.load('embeddings.npy')\n",
    "    else:\n",
    "        embeddings = generate_embeddings(combined_texts, model_name=EMBEDDING_MODEL_NAME)\n",
    "        np.save('embeddings.npy', embeddings)\n",
    "        print(f\"Embeddings saved to 'embeddings.npy'\")\n",
    "\n",
    "    # Step 4: Build FAISS index (optional for re-ranking)\n",
    "    # If you plan to use FAISS for initial retrieval, implement it here.\n",
    "    # For fine-tuning, this might not be necessary.\n",
    "\n",
    "    # Step 5: Load the generation model with LoRA\n",
    "    print(\"Loading generation model with LoRA...\")\n",
    "    tokenizer, model = load_model_with_lora(GENERATION_MODEL_NAME, device)\n",
    "\n",
    "    # Step 6: Prepare the dataset for training\n",
    "    print(\"Preparing the dataset for training...\")\n",
    "    tokenized_dataset = prepare_dataset(df, tokenizer, max_length=MAX_INPUT_LENGTH)\n",
    "\n",
    "    # Step 7: Define training arguments\n",
    "    print(\"Setting up training arguments...\")\n",
    "    training_args = get_training_arguments(OUTPUT_DIR, per_device_train_batch_size=BATCH_SIZE, num_train_epochs=NUM_EPOCHS)\n",
    "\n",
    "    # Step 8: Initialize the Trainer\n",
    "    print(\"Initializing the Trainer...\")\n",
    "    trainer = get_trainer(model, tokenizer, training_args, tokenized_dataset)\n",
    "\n",
    "    # Step 9: Fine-Tune the Model\n",
    "    print(\"Starting fine-tuning...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # Step 10: Save the Fine-Tuned Model\n",
    "    print(\"Saving the fine-tuned model...\")\n",
    "    save_fine_tuned_model(trainer, OUTPUT_DIR)\n",
    "\n",
    "    print(\"Fine-tuning complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
